"""
æµ‹è¯•ç´¢å¼•æ„å»ºæµç¨‹
å¤„ç†å°‘é‡æ•°æ®éªŒè¯ç®¡çº¿æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import sys
from pathlib import Path
import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parents[1]
sys.path.insert(0, str(project_root))

from src.indexing import IndexingPipeline
from src.utils.logger import setup_logger


def test_small_batch():
    """æµ‹è¯•å°æ‰¹é‡æ•°æ®"""
    logger = setup_logger("TestIndexing", level="INFO")
    
    logger.info("=" * 60)
    logger.info("ğŸ§ª æµ‹è¯•ç´¢å¼•æ„å»ºæµç¨‹(å°æ‰¹é‡)")
    logger.info("=" * 60)
    
    try:
        # 1. åŠ è½½å°‘é‡æ•°æ®è¿›è¡Œæµ‹è¯•
        logger.info("\nğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®...")
        data_path = project_root / "data" / "processed" / "93182_steam_games_cleaned.parquet"
        
        if not data_path.exists():
            data_path = project_root / "data" / "processed" / "93182_steam_games_cleaned.csv"
        
        df = pd.read_parquet(data_path) if data_path.suffix == '.parquet' else pd.read_csv(data_path)
        
        # åªå–å‰10æ¡æ•°æ®
        df_test = df.head(10).copy()
        logger.info(f"   æµ‹è¯•æ•°æ®: {len(df_test)} è¡Œ")
        
        # 2. ä¸´æ—¶ä¿å­˜æµ‹è¯•æ•°æ®
        test_parquet = project_root / "data" / "processed" / "test_sample.parquet"
        df_test.to_parquet(test_parquet, index=False)
        logger.info(f"   ä¿å­˜æµ‹è¯•æ•°æ®åˆ°: {test_parquet}")
        
        # 3. åˆå§‹åŒ–ç®¡çº¿
        logger.info("\nğŸ“¦ åˆå§‹åŒ–ç´¢å¼•æ„å»ºç®¡çº¿...")
        pipeline = IndexingPipeline()
        
        # ä¸´æ—¶ä¿®æ”¹é…ç½®æŒ‡å‘æµ‹è¯•æ•°æ®
        pipeline.config['data_source']['input_parquet'] = str(test_parquet)
        pipeline.config['batch_processing']['chunk_size'] = 5  # å°æ‰¹é‡
        
        # 4. æ„å»ºç´¢å¼•
        logger.info("\nğŸš€ å¼€å§‹æ„å»ºæµ‹è¯•ç´¢å¼•...")
        pipeline.build_index(reset=True)
        
        # 5. éªŒè¯ç»“æœ
        logger.info("\nâœ… ç´¢å¼•æ„å»ºå®Œæˆ!")
        count = pipeline.vector_store.get_count()
        logger.info(f"   å‘é‡æ•°é‡: {count}")
        
        # 6. æµ‹è¯•æŸ¥è¯¢
        logger.info("\nğŸ” æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½...")
        results = pipeline.vector_store.query(
            query_texts=["action game"],
            n_results=3
        )
        
        logger.info(f"   æŸ¥è¯¢ç»“æœæ•°: {len(results['ids'][0])}")
        if results['ids'][0]:
            logger.info(f"   å‰3ä¸ªæ¸¸æˆID: {results['ids'][0][:3]}")
            if 'metadatas' in results and results['metadatas'][0]:
                logger.info(f"   ç¬¬ä¸€ä¸ªæ¸¸æˆ: {results['metadatas'][0][0].get('Name', 'Unknown')}")
        
        logger.info("\n" + "=" * 60)
        logger.info("âœ… æµ‹è¯•é€šè¿‡!")
        logger.info("=" * 60)
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        test_parquet.unlink()
        
    except Exception as e:
        logger.error(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    test_small_batch()

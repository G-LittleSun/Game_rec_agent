"""
ç´¢å¼•æ„å»ºç®¡çº¿
æ•°æ®åŠ è½½ â†’ æ¸…æ´— â†’ ç‰¹å¾å·¥ç¨‹ â†’ æ–‡æœ¬èåˆ â†’ Embedding â†’ å…¥åº“
"""
import os
import pandas as pd
import yaml
from pathlib import Path
from typing import Dict, Any, Optional, List
from tqdm import tqdm

from src.data_processing import TextCleaner, FeatureEngineer, DataNormalizer
from src.models.model_manager import get_model_manager
from src.vectordb import ChromaVectorStore
from src.utils.logger import setup_logger


class IndexingPipeline:
    """ç´¢å¼•æ„å»ºç®¡çº¿"""
    
    def __init__(self, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–ç´¢å¼•æ„å»ºç®¡çº¿
        
        Args:
            config_path: vectorization.yamlé…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.logger = setup_logger("IndexingPipeline")
        
        # åŠ è½½é…ç½®
        if config_path is None:
            config_path = Path(__file__).parents[2] / "config" / "vectorization.yaml"
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.text_cleaner = TextCleaner(
            **self.config['text_fusion']['about_cleaning']
        )
        
        self.feature_engineer = FeatureEngineer(
            config=self.config.get('feature_engineering', {})
        )
        
        self.normalizer = DataNormalizer(
            default_values=self.config['text_fusion']['default_values']
        )
        
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        self.model_manager = get_model_manager()
        
        # åˆå§‹åŒ–å‘é‡åº“
        from src.config.model_config import get_config
        model_cfg = get_config()
        vdb_cfg = model_cfg.vectordb
        
        self.vector_store = ChromaVectorStore(
            persist_directory=vdb_cfg['persist_directory'],
            collection_name=self.config['vectordb']['collection_name'],
            distance_metric=self.config['vectordb']['distance_metric']
        )
        
        self.logger.info("âœ… ç´¢å¼•æ„å»ºç®¡çº¿åˆå§‹åŒ–å®Œæˆ")
    
    def build_index(self, reset: bool = False) -> None:
        """
        æ„å»ºå®Œæ•´ç´¢å¼•
        
        Args:
            reset: æ˜¯å¦æ¸…ç©ºå·²æœ‰ç´¢å¼•é‡æ–°æ„å»º
        """
        if reset:
            self.logger.warning("âš ï¸ æ¸…ç©ºç°æœ‰ç´¢å¼•...")
            self.vector_store.reset()
            # é‡æ–°åˆ›å»ºé›†åˆ
            from src.config.model_config import get_config
            model_cfg = get_config()
            vdb_cfg = model_cfg.vectordb
            self.vector_store = ChromaVectorStore(
                persist_directory=vdb_cfg['persist_directory'],
                collection_name=self.config['vectordb']['collection_name'],
                distance_metric=self.config['vectordb']['distance_metric']
            )
        
        # 1. åŠ è½½æ•°æ®
        self.logger.info("ğŸ“‚ åŠ è½½æ•°æ®...")
        df = self._load_data()
        self.logger.info(f"   æ•°æ®è¡Œæ•°: {len(df)}")
        
        # 2. æ•°æ®é¢„å¤„ç†
        self.logger.info("ğŸ”§ æ•°æ®é¢„å¤„ç†...")
        df = self._preprocess_data(df)
        
        # 3. ç‰¹å¾å·¥ç¨‹
        self.logger.info("âš™ï¸ ç‰¹å¾å·¥ç¨‹...")
        df = self._engineer_features(df)
        
        # 4. æ–‡æœ¬èåˆ
        self.logger.info("ğŸ“ æ–‡æœ¬èåˆ...")
        df = self._fuse_text(df)
        
        # 5. å‡†å¤‡metadata
        self.logger.info("ğŸ“¦ å‡†å¤‡metadata...")
        metadatas = self._prepare_metadatas(df)
        
        # 6. Embedding + å…¥åº“
        self.logger.info("ğŸ”¢ Embeddingå¹¶å…¥åº“...")
        self._embed_and_index(df, metadatas)
        
        # 7. æŒä¹…åŒ–
        self.vector_store.persist()
        
        self.logger.info(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ! æ€»æ–‡æ¡£æ•°: {self.vector_store.get_count()}")
    
    def _load_data(self) -> pd.DataFrame:
        """åŠ è½½æ•°æ®"""
        data_cfg = self.config['data_source']
        
        # ä¼˜å…ˆåŠ è½½parquet(æ›´å¿«)
        parquet_path = data_cfg['input_parquet']
        csv_path = data_cfg['input_csv']
        
        if os.path.exists(parquet_path):
            self.logger.info(f"   ä»ParquetåŠ è½½: {parquet_path}")
            return pd.read_parquet(parquet_path)
        elif os.path.exists(csv_path):
            self.logger.info(f"   ä»CSVåŠ è½½: {csv_path}")
            return pd.read_csv(csv_path)
        else:
            raise FileNotFoundError(
                f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {parquet_path} æˆ– {csv_path}"
            )
    
    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®é¢„å¤„ç†"""
        # 1. æ ‡å‡†åŒ–
        df = self.normalizer.normalize_dataframe(df)
        
        # 2. æ¸…æ´—About the game
        if 'About the game' in df.columns:
            self.logger.info("   æ¸…æ´—æ¸¸æˆæè¿°æ–‡æœ¬...")
            df['About_cleaned'] = df['About the game'].apply(
                self.text_cleaner.clean
            )
        else:
            df['About_cleaned'] = ''
        
        return df
    
    def _engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾å·¥ç¨‹"""
        rating_cfg = self.config['feature_engineering']['rating']
        pop_cfg = self.config['feature_engineering']['popularity']
        quality_cfg = self.config['feature_engineering']['quality']
        
        # 1. è®¡ç®—è¯„åˆ†
        if 'Positive' in df.columns and 'Negative' in df.columns:
            self.logger.info("   è®¡ç®—ç”¨æˆ·è¯„åˆ†...")
            df['final_rating'] = self.feature_engineer.compute_rating(
                df['Positive'],
                df['Negative'],
                method=rating_cfg['method'],
                confidence=rating_cfg['confidence']
            )
        else:
            df['final_rating'] = 0.5
        
        # 2. è®¡ç®—çƒ­åº¦è¯„åˆ†
        self.logger.info("   è®¡ç®—çƒ­åº¦è¯„åˆ†...")
        df['popularity_score'] = self.feature_engineer.compute_popularity_score(
            df,
            weights=pop_cfg['weights']
        )
        
        # 3. è®¡ç®—è´¨é‡è¯„åˆ†
        self.logger.info("   è®¡ç®—è´¨é‡è¯„åˆ†...")
        df['quality_score'] = self.feature_engineer.compute_quality_score(
            df,
            weights=quality_cfg['weights']
        )
        
        return df
    
    def _fuse_text(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ–‡æœ¬èåˆ - æŒ‰æ¨¡æ¿æ‹¼æ¥"""
        template = self.config['text_fusion']['template']
        
        self.logger.info("   åº”ç”¨æ–‡æœ¬èåˆæ¨¡æ¿...")
        
        def fuse_single(row):
            try:
                return template.format(
                    Name=str(row.get('Name', 'Unknown')),
                    About_cleaned=str(row.get('About_cleaned', ''))[:600],  # é™åˆ¶é•¿åº¦
                    Genres=str(row.get('Genres', 'Unknown')),
                    Tags=str(row.get('Tags', 'Unknown')),
                    Categories=str(row.get('Categories', 'Unknown')),
                    Release_year=str(row.get('Release_year', 'Unknown')),
                    Platforms=str(row.get('Platforms', 'Unknown')),
                    popularity_score=float(row.get('popularity_score', 0)),
                    quality_score=float(row.get('quality_score', 0))
                )
            except Exception as e:
                self.logger.warning(f"   æ–‡æœ¬èåˆå¤±è´¥: {e}")
                return f"Name: {row.get('Name', 'Unknown')}"
        
        df['text_combined'] = df.apply(fuse_single, axis=1)
        
        return df
    
    def _prepare_metadatas(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """å‡†å¤‡metadataå­—å…¸åˆ—è¡¨"""
        meta_cfg = self.config['metadata_fields']
        
        # æ”¶é›†æ‰€æœ‰è¦ä¿å­˜çš„å­—æ®µ
        all_fields = set()
        for field_list in meta_cfg.values():
            if isinstance(field_list, list):
                all_fields.update(field_list)
        
        metadatas = []
        
        for _, row in df.iterrows():
            meta = {}
            
            for field in all_fields:
                if field in row:
                    value = row[field]
                    
                    # å¤„ç†NaNå’ŒNone
                    if pd.isna(value):
                        continue
                    
                    # è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–ç±»å‹
                    if isinstance(value, (int, float, bool, str)):
                        meta[field] = value
                    else:
                        meta[field] = str(value)
            
            metadatas.append(meta)
        
        return metadatas
    
    def _embed_and_index(
        self,
        df: pd.DataFrame,
        metadatas: List[Dict[str, Any]]
    ) -> None:
        """Embeddingå¹¶æ‰¹é‡å…¥åº“"""
        batch_cfg = self.config['batch_processing']
        chunk_size = batch_cfg['chunk_size']
        show_progress = batch_cfg['show_progress']
        
        # å‡†å¤‡æ•°æ®
        ids = df['AppID'].astype(str).tolist()
        documents = df['text_combined'].tolist()
        
        # æ‰¹é‡å¤„ç†
        total_batches = (len(documents) + chunk_size - 1) // chunk_size
        
        iterator = range(0, len(documents), chunk_size)
        if show_progress:
            iterator = tqdm(iterator, total=total_batches, desc="Embedding")
        
        for i in iterator:
            batch_ids = ids[i:i + chunk_size]
            batch_docs = documents[i:i + chunk_size]
            batch_metas = metadatas[i:i + chunk_size]
            
            # Embedding
            embeddings = self.model_manager.embedding.encode_corpus(
                batch_docs,
                show_progress=False
            )
            
            # å…¥åº“
            self.vector_store.add(
                ids=batch_ids,
                documents=batch_docs,
                metadatas=batch_metas,
                embeddings=embeddings
            )
            
            # ä¿å­˜æ£€æŸ¥ç‚¹(å¯é€‰)
            if batch_cfg.get('save_checkpoint', False):
                if (i + chunk_size) % batch_cfg.get('checkpoint_interval', 1000) == 0:
                    self.logger.info(f"   æ£€æŸ¥ç‚¹: å·²å¤„ç† {i + chunk_size} æ¡")


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    pipeline = IndexingPipeline()
    
    # æ„å»ºç´¢å¼•(æ¸…ç©ºé‡å»º)
    pipeline.build_index(reset=True)

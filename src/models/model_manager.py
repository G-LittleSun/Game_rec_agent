"""
æ¨¡å‹ç®¡ç†å™¨
ç»Ÿä¸€ç®¡ç†LLMã€Embeddingã€Rerankæ¨¡å‹,æä¾›ç®€æ´çš„è°ƒç”¨æ¥å£
"""
from typing import List, Union, Optional, Dict, Any
from src.config.model_config import get_config
from src.models.ollama_client import OllamaClient
from src.models.embedding_client import EmbeddingClient
from src.models.rerank_client import RerankClient
from src.utils.logger import setup_logger


class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ - ç»Ÿä¸€å…¥å£"""
    
    def __init__(self, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„,ä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        # åŠ è½½é…ç½®
        self.config = get_config(config_path)
        self.logger = setup_logger(
            "ModelManager",
            level=self.config.logging.get("level", "INFO"),
            log_file=self.config.logging.get("log_file")
        )
        
        # åˆå§‹åŒ–å„æ¨¡å‹å®¢æˆ·ç«¯
        self._llm_client = None
        self._embedding_client = None
        self._rerank_client = None
        
        self.logger.info("âœ… æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    @property
    def llm(self) -> OllamaClient:
        """è·å–LLMå®¢æˆ·ç«¯(æ‡’åŠ è½½)"""
        if self._llm_client is None:
            llm_config = self.config.llm
            provider = llm_config.get("provider", "ollama")
            
            if provider == "ollama":
                ollama_cfg = llm_config["ollama"]
                self._llm_client = OllamaClient(
                    base_url=ollama_cfg["base_url"],
                    model=ollama_cfg["model"],
                    temperature=ollama_cfg.get("temperature", 0.7),
                    top_p=ollama_cfg.get("top_p", 0.9),
                    max_tokens=ollama_cfg.get("max_tokens", 2048),
                    timeout=ollama_cfg.get("timeout", 120)
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„LLM provider: {provider}")
        
        return self._llm_client
    
    @property
    def embedding(self) -> EmbeddingClient:
        """è·å–Embeddingå®¢æˆ·ç«¯(æ‡’åŠ è½½)"""
        if self._embedding_client is None:
            emb_config = self.config.embedding
            self._embedding_client = EmbeddingClient(
                model_name=emb_config["model_name"],
                device=emb_config.get("device", "cuda"),
                batch_size=emb_config.get("batch_size", 32),
                max_length=emb_config.get("max_length", 512),
                normalize_embeddings=emb_config.get("normalize_embeddings", True),
                cache_dir=emb_config.get("cache_dir", "./data/model_cache")
            )
        
        return self._embedding_client
    
    @property
    def rerank(self) -> RerankClient:
        """è·å–Rerankå®¢æˆ·ç«¯(æ‡’åŠ è½½)"""
        if self._rerank_client is None:
            rerank_config = self.config.rerank
            self._rerank_client = RerankClient(
                model_name=rerank_config["model_name"],
                device=rerank_config.get("device", "cuda"),
                batch_size=rerank_config.get("batch_size", 16),
                max_length=rerank_config.get("max_length", 512),
                cache_dir=rerank_config.get("cache_dir", "./data/model_cache")
            )
        
        return self._rerank_client
    
    # ==================== ä¾¿æ·æ–¹æ³• ====================
    
    def generate_text(
        self,
        prompt: str,
        system: Optional[str] = None,
        stream: bool = False,
        **kwargs
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬(LLM)
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥
            system: ç³»ç»Ÿæç¤ºè¯
            stream: æ˜¯å¦æµå¼è¿”å›
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        return self.llm.generate(prompt, system=system, stream=stream, **kwargs)
    
    def encode_text(
        self,
        texts: Union[str, List[str]],
        is_query: bool = False,
        **kwargs
    ) -> Any:
        """
        æ–‡æœ¬ç¼–ç ä¸ºå‘é‡
        
        Args:
            texts: æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            is_query: æ˜¯å¦ä¸ºæŸ¥è¯¢æ–‡æœ¬(ä¼šæ·»åŠ ç‰¹æ®Šå‰ç¼€)
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            æ–‡æœ¬å‘é‡
        """
        if is_query:
            return self.embedding.encode_queries(texts, **kwargs)
        else:
            return self.embedding.encode_corpus(texts if isinstance(texts, list) else [texts], **kwargs)
    
    def rerank_documents(
        self,
        query: str,
        documents: List[str],
        top_k: int = 10,
        **kwargs
    ) -> List[tuple]:
        """
        å¯¹æ–‡æ¡£è¿›è¡Œé‡æ’åº
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: å€™é€‰æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›å‰kä¸ªç»“æœ
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            é‡æ’åºåçš„ç»“æœ [(document, score), ...]
        """
        return self.rerank.rerank_with_docs(query, documents, top_k=top_k, **kwargs)
    
    def reload_config(self):
        """é‡æ–°åŠ è½½é…ç½®"""
        self.config.reload()
        # é‡ç½®å®¢æˆ·ç«¯,ä¸‹æ¬¡ä½¿ç”¨æ—¶ä¼šæ ¹æ®æ–°é…ç½®é‡æ–°åˆå§‹åŒ–
        self._llm_client = None
        self._embedding_client = None
        self._rerank_client = None
        self.logger.info("âœ… é…ç½®å·²é‡æ–°åŠ è½½")
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰æ¨¡å‹ä¿¡æ¯"""
        info = {
            "llm": {
                "provider": self.config.llm.get("provider"),
                "model": self.config.llm.get("ollama", {}).get("model"),
            },
            "embedding": {
                "model": self.config.embedding.get("model_name"),
                "device": self.config.embedding.get("device"),
                "dimension": self.embedding.get_embedding_dim() if self._embedding_client else None
            },
            "rerank": {
                "model": self.config.rerank.get("model_name"),
                "device": self.config.rerank.get("device"),
            }
        }
        return info


# å…¨å±€å•ä¾‹
_global_manager = None


def get_model_manager(config_path: Optional[str] = None) -> ModelManager:
    """
    è·å–å…¨å±€æ¨¡å‹ç®¡ç†å™¨å®ä¾‹(å•ä¾‹)
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„,ä»…é¦–æ¬¡è°ƒç”¨æ—¶æœ‰æ•ˆ
    
    Returns:
        ModelManagerå®ä¾‹
    """
    global _global_manager
    if _global_manager is None:
        _global_manager = ModelManager(config_path)
    return _global_manager


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # åˆ›å»ºç®¡ç†å™¨
    manager = ModelManager()
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print("ğŸ“Š å½“å‰æ¨¡å‹é…ç½®:")
    import json
    print(json.dumps(manager.get_model_info(), indent=2, ensure_ascii=False))
    
    # æµ‹è¯•LLM
    print("\nğŸ¤– æµ‹è¯•LLM:")
    response = manager.generate_text(
        prompt="ç®€å•ä»‹ç»ä¸€ä¸‹ã€Šå¡å°”è¾¾ä¼ è¯´:æ—·é‡ä¹‹æ¯ã€‹",
        system="ä½ æ˜¯ä¸€ä¸ªæ¸¸æˆæ¨èä¸“å®¶"
    )
    print(response)
    
    # æµ‹è¯•Embedding
    print("\nğŸ”¢ æµ‹è¯•Embedding:")
    texts = ["å¼€æ”¾ä¸–ç•Œæ¸¸æˆ", "RPGæ¸¸æˆ", "å°„å‡»æ¸¸æˆ"]
    embeddings = manager.encode_text(texts)
    print(f"å‘é‡å½¢çŠ¶: {embeddings.shape}")
    
    # æµ‹è¯•Rerank
    print("\nğŸ”„ æµ‹è¯•Rerank:")
    query = "æ¨èå¼€æ”¾ä¸–ç•Œæ¸¸æˆ"
    docs = [
        "ã€Šå¡å°”è¾¾ä¼ è¯´ã€‹æ˜¯ä¸€æ¬¾å¼€æ”¾ä¸–ç•Œå†’é™©æ¸¸æˆ",
        "ã€Šåæç²¾è‹±ã€‹æ˜¯ä¸€æ¬¾ç¬¬ä¸€äººç§°å°„å‡»æ¸¸æˆ",
        "ã€Šè‰¾å°”ç™»æ³•ç¯ã€‹æ˜¯å¼€æ”¾ä¸–ç•ŒRPG"
    ]
    results = manager.rerank_documents(query, docs, top_k=2)
    for doc, score in results:
        print(f"[{score:.4f}] {doc}")
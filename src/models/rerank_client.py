"""
Rerank æ¨¡å‹å®¢æˆ·ç«¯
ä½¿ç”¨ CrossEncoder å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº
"""
import torch
import numpy as np
from typing import List, Tuple
from sentence_transformers import CrossEncoder
from src.utils.logger import setup_logger


class RerankClient:
    """Rerankæ¨¡å‹å®¢æˆ·ç«¯"""
    
    def __init__(
        self,
        model_name: str = "BAAI/bge-reranker-large",
        device: str = "cuda",
        batch_size: int = 16,
        max_length: int = 512,
        cache_dir: str = "./data/model_cache"
    ):
        """
        åˆå§‹åŒ–Rerankå®¢æˆ·ç«¯
        
        Args:
            model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
            device: è¿è¡Œè®¾å¤‡ (cuda/cpu/mps)
            batch_size: æ‰¹å¤„ç†å¤§å°
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
        """
        self.model_name = model_name
        self.batch_size = batch_size
        self.max_length = max_length
        
        # è®¾ç½®è®¾å¤‡
        if device == "cuda" and not torch.cuda.is_available():
            device = "cpu"
            print("âš ï¸ CUDAä¸å¯ç”¨,ä½¿ç”¨CPU")
        elif device == "mps" and not torch.backends.mps.is_available():
            device = "cpu"
            print("âš ï¸ MPSä¸å¯ç”¨,ä½¿ç”¨CPU")
        
        self.device = device

        
        self.logger = setup_logger("RerankClient")
        
        # åŠ è½½æ¨¡å‹
        self.logger.info(f"ğŸ”„ åŠ è½½Rerankæ¨¡å‹: {model_name}")
        self.model = CrossEncoder(
            model_name,
            max_length=max_length,
            device=device,
            default_activation_function=torch.nn.Sigmoid()  # è¾“å‡º0-1ä¹‹é—´çš„åˆ†æ•°
        )
        self.logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ,è®¾å¤‡: {device}")
    
    def rerank(
        self,
        query: str,
        documents: List[str],
        top_k: int = None,
        return_scores: bool = True,
        batch_size: int = None
    ) -> List[Tuple[int, float]]:
        """
        å¯¹æ–‡æ¡£è¿›è¡Œé‡æ’åº
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: å€™é€‰æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›å‰kä¸ªç»“æœ,ä¸ºNoneåˆ™è¿”å›å…¨éƒ¨
            return_scores: æ˜¯å¦è¿”å›åˆ†æ•°
            batch_size: æ‰¹å¤„ç†å¤§å°
        
        Returns:
            é‡æ’åºåçš„ç»“æœåˆ—è¡¨ [(doc_index, score), ...]
        """
        if batch_size is None:
            batch_size = self.batch_size
        
        # æ„é€ query-documentå¯¹
        pairs = [[query, doc] for doc in documents]
        
        # é¢„æµ‹ç›¸å…³æ€§åˆ†æ•°
        scores = self.model.predict(
            pairs,
            batch_size=batch_size,
            show_progress_bar=False,
            convert_to_numpy=True
        )
        
        # æ’åº
        ranked_results = [
            (idx, float(score))
            for idx, score in enumerate(scores)
        ]
        ranked_results.sort(key=lambda x: x[1], reverse=True)
        
        # æˆªæ–­top_k
        if top_k is not None:
            ranked_results = ranked_results[:top_k]
        
        if return_scores:
            return ranked_results
        else:
            return [idx for idx, _ in ranked_results]
    
    def rerank_with_docs(
        self,
        query: str,
        documents: List[str],
        top_k: int = None,
        batch_size: int = None
    ) -> List[Tuple[str, float]]:
        """
        å¯¹æ–‡æ¡£è¿›è¡Œé‡æ’åºå¹¶è¿”å›æ–‡æ¡£å†…å®¹
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: å€™é€‰æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›å‰kä¸ªç»“æœ
            batch_size: æ‰¹å¤„ç†å¤§å°
        
        Returns:
            é‡æ’åºåçš„ç»“æœåˆ—è¡¨ [(document, score), ...]
        """
        ranked_indices = self.rerank(
            query,
            documents,
            top_k=top_k,
            return_scores=True,
            batch_size=batch_size
        )
        
        return [
            (documents[idx], score)
            for idx, score in ranked_indices
        ]
    
    def score(
        self,
        query_doc_pairs: List[Tuple[str, str]],
        batch_size: int = None
    ) -> np.ndarray:
        """
        ç›´æ¥è®¡ç®—query-documentå¯¹çš„ç›¸å…³æ€§åˆ†æ•°
        
        Args:
            query_doc_pairs: (query, document)å¯¹åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
        
        Returns:
            ç›¸å…³æ€§åˆ†æ•°æ•°ç»„
        """
        if batch_size is None:
            batch_size = self.batch_size
        
        pairs = [[q, d] for q, d in query_doc_pairs]
        scores = self.model.predict(
            pairs,
            batch_size=batch_size,
            show_progress_bar=False,
            convert_to_numpy=True
        )
        
        return scores
    
    def switch_model(self, model_name: str):
        """åˆ‡æ¢æ¨¡å‹"""
        self.logger.info(f"ğŸ”„ åˆ‡æ¢Rerankæ¨¡å‹: {model_name}")
        
        self.model_name = model_name
        self.model = CrossEncoder(
            model_name,
            max_length=self.max_length,
            device=self.device,
            default_activation_function=torch.nn.Sigmoid()
        )
        self.logger.info(f"âœ… æ¨¡å‹åˆ‡æ¢å®Œæˆ")


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = RerankClient(
        model_name="BAAI/bge-reranker-base",
        device="cpu"  # æµ‹è¯•ç”¨CPU
    )
    
    # å‡†å¤‡æ•°æ®
    query = "æ¨èä¸€æ¬¾å¼€æ”¾ä¸–ç•ŒRPGæ¸¸æˆ"
    documents = [
        "ã€Šå¡å°”è¾¾ä¼ è¯´:æ—·é‡ä¹‹æ¯ã€‹æ˜¯ä¸€æ¬¾å¼€æ”¾ä¸–ç•Œå†’é™©æ¸¸æˆ,æ‹¥æœ‰å¹¿é˜”çš„æ¢ç´¢ç©ºé—´",
        "ã€Šè‰¾å°”ç™»æ³•ç¯ã€‹æ˜¯FromSoftwareå¼€å‘çš„å¼€æ”¾ä¸–ç•ŒåŠ¨ä½œRPGæ¸¸æˆ",
        "ã€Šæˆ‘çš„ä¸–ç•Œã€‹æ˜¯ä¸€æ¬¾æ²™ç›’å»ºé€ æ¸¸æˆ",
        "ã€Šè¶…çº§é©¬åŠ›æ¬§:å¥¥å¾·èµ›ã€‹æ˜¯ä¸€æ¬¾3Då¹³å°è·³è·ƒæ¸¸æˆ",
        "ã€Šå·«å¸ˆ3:ç‹‚çŒã€‹æ˜¯ä¸€æ¬¾ä¸­ä¸–çºªå¥‡å¹»å¼€æ”¾ä¸–ç•ŒRPG"
    ]
    
    # é‡æ’åº
    results = client.rerank_with_docs(query, documents, top_k=3)
    
    print(f"æŸ¥è¯¢: {query}\n")
    print("é‡æ’åºç»“æœ:")
    for i, (doc, score) in enumerate(results, 1):
        print(f"{i}. [åˆ†æ•°: {score:.4f}] {doc}")